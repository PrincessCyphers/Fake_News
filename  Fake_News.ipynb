{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa26dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # data processing \n",
    "import os # access to dir structure\n",
    "import matplotlib.pyplot as plt # plotting \n",
    "import numpy as np # linear algebra \n",
    "import seaborn as sns \n",
    "\n",
    "# import kaggle "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762a4004",
   "metadata": {},
   "source": [
    "Dataset in use \n",
    "https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset?select=True.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4905d86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view files in downloaded dataset folder \n",
    "\n",
    "csv_files = os.listdir(r\"/Users/Taurai/Iza/CFG/Data Science : Part 2/Project Folder /archive(3)\")\n",
    "for file in csv_files: \n",
    "    print(file)\n",
    "print(csv_files.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2faebc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_df1 = pd.read_csv(\"./archive(3)/Fake.csv\")\n",
    "real_df1 = pd.read_csv(\"./archive(3)/True.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a83617",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_df1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16247684",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_df1.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c0a92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_df1['subject'].value_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9f2807",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_df1['subject'].unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28a3a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fake_df1.shape)\n",
    "print(real_df1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942e65a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label the diff df for easier ref\n",
    "\n",
    "fake_df1['true'] = 0\n",
    "real_df1['true'] = 1\n",
    "\n",
    "# print new shape \n",
    "print(fake_df1.shape)\n",
    "print(real_df1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7f4f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just to see \n",
    "\n",
    "real_df1.describe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36c2bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat the two datasets  \n",
    "\n",
    "raw_df = pd.concat([fake_df1, real_df1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c79c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32144f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.true.value_counts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1d9ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting by subject\n",
    "\n",
    "for key, count in raw_df['subject'].value_counts().iteritems():\n",
    "    print(f\"{key}:\\t{count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6be246",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# spread of information sources/ subjects in raw df \n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.countplot('subject', data = raw_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c9f5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word cloud of fake vs real df for quick comparison before cleaning and processing \n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import nltk\n",
    "\n",
    "\n",
    "text = ''\n",
    "for news in fake_df1.text.values:\n",
    "    text += f\" {news}\"\n",
    "wordcloud = WordCloud(\n",
    "    width = 750, height = 400, \n",
    "    background_color = 'purple', \n",
    "    stopwords=set(nltk.corpus.stopwords.words(\"english\"))).generate(text)\n",
    "fig = plt.figure(\n",
    "    figsize = (20,15), \n",
    "    facecolor = 'm',\n",
    "    edgecolor = 'm')\n",
    "plt.imshow(wordcloud, interpolation = 'bilinear')\n",
    "plt.axis('off')\n",
    "plt.show\n",
    "del text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b76767f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real news word cloud \n",
    "\n",
    "text = ''\n",
    "for news in real_df1.text.values:\n",
    "    text += f\" {news}\"\n",
    "wordcloud = WordCloud(\n",
    "    width = 750, height = 400, \n",
    "    background_color = 'black', \n",
    "    stopwords=set(nltk.corpus.stopwords.words(\"english\"))).generate(text)\n",
    "fig = plt.figure(\n",
    "    figsize = (20,15), \n",
    "    facecolor = 'm',\n",
    "    edgecolor = 'm')\n",
    "plt.imshow(wordcloud, interpolation = 'bilinear')\n",
    "plt.axis('off')\n",
    "plt.show\n",
    "del text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c873bcb",
   "metadata": {},
   "source": [
    "## Data Cleaning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e36c05",
   "metadata": {},
   "source": [
    "Df needs cleaning. Ultimate Goal is:  \n",
    "* Check for null values\n",
    "* check for empty spaces \n",
    "* check for nonsense texts \n",
    "* remove urls, https, special characters? \n",
    "* remove REUTERS()\n",
    "* group text by subject? and label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b82f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check null values\n",
    "raw_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edce2fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop missing values\n",
    "\n",
    "raw_df = raw_df.dropna()\n",
    "raw_df.shape\n",
    "\n",
    "# Note, dropna() not picking up all null values as some text missing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7364872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make [index] for rows that don't have publication info (REUTERS)\n",
    "\n",
    "\n",
    "no_publisher = []\n",
    "for index, row in enumerate(raw_df.text.values):\n",
    "    try:\n",
    "        record = row.split(\" -\", maxsplit=1)\n",
    "        \n",
    "        # if no text present, this should raise error \n",
    "        record[1]\n",
    "        assert(len(record[0]) < 260)\n",
    "    except:\n",
    "        no_publisher.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a5ac80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for the 'clean' data, that doesn't start with REUTERS\n",
    "raw_df.iloc[no_publisher].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd6da2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of indexes for rows that have publication info (REUTERS)\n",
    "# this will seperate the text from the publisher info (i.e seperate REUTERS())\n",
    "\n",
    "publisher = []\n",
    "pt_text=[]\n",
    "\n",
    "for index, row in enumerate(raw_df.text.values):\n",
    "    if index in no_publisher: \n",
    "        \n",
    "#         if no publisher mentioned, add unknown\n",
    "        pt_text.append(row)\n",
    "        publisher.append(\"Unknown\")\n",
    "        continue \n",
    "        \n",
    "    record = row.split(\" -\", maxsplit=1)\n",
    "    publisher.append(record[0])\n",
    "    pt_text.append(record[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbe964c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace current text column with the new one made above \n",
    "# add new seperate column for publisher info (i.e REUTERS())\n",
    "\n",
    "raw_df[\"publisher\"] = publisher \n",
    "raw_df[\"text\"] = pt_text\n",
    "\n",
    "del publisher, pt_text, record, no_publisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccdcb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51087d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.tail(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e95dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for empty rows and list them\n",
    "\n",
    "empty = [index for index, text in enumerate(raw_df.text.values) if str(text).strip()=='']\n",
    "print(f\"Number of empty rows: {len(empty)}\")\n",
    "raw_df.iloc[empty].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90eb413a",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a18232b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  drop these empty row records \n",
    "\n",
    "# raw_df = raw_df.drop(empty, axis=0)\n",
    "# raw_df = raw_df[raw_df['text'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7951c460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use numpy to replace empty with NaN function in order to pick up using dropna()\n",
    "\n",
    "raw_df['text'].replace(' ', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ba234a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now can drop the null values \n",
    "\n",
    "raw_df = raw_df.dropna(subset=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88a60f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec7bc20",
   "metadata": {},
   "source": [
    "### Why isn't dropna working?  \n",
    "\n",
    "it now is after adding white space to empty text str."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48494ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm no empty rows and not white spaces once more \n",
    "# note lost more rows \n",
    "\n",
    "raw_df = raw_df.drop([index for index, text in enumerate(raw_df.text.values) if str(text).strip()==''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed6dd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c802a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text has bool true so should return false if empty \n",
    "\n",
    "raw_df['text'].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f012896a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# raw_df = raw_df.drop(empty, inplace=True)\n",
    "# raw_df['text'].str.strip().astype(bool)\n",
    "# raw_df['text'].astype(bool)\n",
    "# df.dropna()\n",
    "# raw_df.iloc[empty].tail()\n",
    "# raw_df = (raw_df.iloc[empty]).replace('', np.nan)\n",
    "# print(raw_df)\n",
    "# print(type(df))\n",
    "# raw_df.empty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7aa1a5",
   "metadata": {},
   "source": [
    "## Let's clean up the DF "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca46f1b",
   "metadata": {},
   "source": [
    "Delete unecessary rows - publisher, date, title, subject \n",
    "They won't be needed going forwards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d9a404",
   "metadata": {},
   "outputs": [],
   "source": [
    "del raw_df['title']\n",
    "del raw_df['subject']\n",
    "del raw_df['date']\n",
    "del raw_df['publisher']\n",
    "\n",
    "\n",
    "raw_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ba58ec",
   "metadata": {},
   "source": [
    "### Lets Clean up the data\n",
    "\n",
    "* Remove punctuation\n",
    "* remove special char \n",
    "* convert upper to lower \n",
    "* remove stopwords\n",
    "* remove urls\n",
    "* Lemmatization - The stemming of words without loss of meaning to context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1422643",
   "metadata": {},
   "source": [
    "We'll clean the first news article only for now... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d435ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_1 = raw_df['text'][0]\n",
    "type(text_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246f6b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  use contractions lib for context when expanding contractions (i'd -> i would)\n",
    "import contractions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e189b064",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_1 = ' '.join([contractions.fix(word) for word in text_1.str.split()])\n",
    "text_1\n",
    "\n",
    "# will tokenizer work instead? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6cb083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove special characters and punctuation \n",
    "import re \n",
    "\n",
    "\n",
    "text_1 = re.sub('\\[[^]]*\\]', ' ', text_1)\n",
    "text_1 = re.sub('[^a-zA-Z]', ' ', text_1)\n",
    "\n",
    "#  convert from lower to upper \n",
    "text_1 = text_1.lower()\n",
    "\n",
    "text_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d730c30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  remove stopwords \n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "text_1 = nltk.word_tokenize(text_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6c2d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_1 = [ word for word in text_1 if not word in set(stopwords.words(\"english\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bef8b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  removal of HTML content\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(text_1, \"html.parser\")\n",
    "text_1 = soup.get_text()\n",
    "text_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddda257d",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(text_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb178ba9",
   "metadata": {},
   "source": [
    "# remove emojies \n",
    "# sentiment analysis - says if dataset is positive, neg or neutral\n",
    "# top to beck groups text into categories (topic model?)1\n",
    "# look at stopwords being removed and make sure list is inclusive \n",
    "# dashboard for visualisation (powerbi)\n",
    "# word cloud for after \n",
    "# pull out a few words when describing \n",
    "# use time frames to compare e.g. most pop topic in feb 2020 was\n",
    "# 30% train and 70% test for ML \n",
    "# topic model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
