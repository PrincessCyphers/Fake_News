{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b87f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # data processing \n",
    "import os # access to dir structure\n",
    "import matplotlib.pyplot as plt # plotting \n",
    "import numpy as np # linear algebra \n",
    "import seaborn as sns \n",
    "\n",
    "# import kaggle "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37df7463",
   "metadata": {},
   "source": [
    "Dataset in use \n",
    "https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset?select=True.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2698b514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view files in downloaded dataset folder \n",
    "\n",
    "csv_files = os.listdir(r\"/Users/Taurai/Iza/CFG/Data Science : Part 2/Project Folder /archive(3)\")\n",
    "for file in csv_files: \n",
    "    print(file)\n",
    "print(csv_files.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6554c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_df1 = pd.read_csv(\"./archive(3)/Fake.csv\")\n",
    "real_df1 = pd.read_csv(\"./archive(3)/True.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca4e366",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_df1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13f6910",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2388bd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_df1.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f811fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_df1['subject'].value_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e13727d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_df1['subject'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c18708",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fake_df1.shape)\n",
    "print(real_df1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8d402d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label the diff df for easier ref\n",
    "\n",
    "fake_df1['true'] = 0\n",
    "real_df1['true'] = 1\n",
    "\n",
    "# print new shape \n",
    "print(fake_df1.shape)\n",
    "print(real_df1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ee76ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just to see \n",
    "\n",
    "real_df1.describe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a47c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat the two datasets  \n",
    "\n",
    "raw_df = pd.concat([fake_df1, real_df1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cd7393",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbb1050",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.true.value_counts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becc5d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting by subject\n",
    "\n",
    "for key, count in raw_df['subject'].value_counts().iteritems():\n",
    "    print(f\"{key}:\\t{count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e473dd0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# spread of information sources/ subjects in raw df \n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.countplot('subject', data = raw_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c3e233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# move until found the right place to position/ when needed import plotly.express as px\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "# colored output text\n",
    "from termcolor import colored\n",
    "import re\n",
    "\n",
    "# display and store Matplotlib plots within a Python Jupyter notebook (no need to use .show())\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca0707f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word cloud of fake vs real df for quick comparison before cleaning and processing \n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import nltk\n",
    "\n",
    "\n",
    "text = ''\n",
    "for news in fake_df1.text.values:\n",
    "    text += f\" {news}\"\n",
    "wordcloud = WordCloud(\n",
    "    width = 750, height = 400, \n",
    "    background_color = 'purple', \n",
    "    stopwords=set(nltk.corpus.stopwords.words(\"english\"))).generate(text)\n",
    "fig = plt.figure(\n",
    "    figsize = (20,15), \n",
    "    facecolor = 'm',\n",
    "    edgecolor = 'm')\n",
    "plt.imshow(wordcloud, interpolation = 'bilinear')\n",
    "plt.axis('off')\n",
    "plt.show\n",
    "del text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3400b9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real news word cloud \n",
    "\n",
    "text = ''\n",
    "for news in real_df1.text.values:\n",
    "    text += f\" {news}\"\n",
    "wordcloud = WordCloud(\n",
    "    width = 750, height = 400, \n",
    "    background_color = 'white', \n",
    "    stopwords=set(nltk.corpus.stopwords.words(\"english\"))).generate(text)\n",
    "fig = plt.figure(\n",
    "    figsize = (20,15), \n",
    "    facecolor = 'm',\n",
    "    edgecolor = 'm')\n",
    "plt.imshow(wordcloud, interpolation = 'bilinear')\n",
    "plt.axis('off')\n",
    "plt.show\n",
    "del text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280319bd",
   "metadata": {},
   "source": [
    "## Data Cleaning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0ce17c",
   "metadata": {},
   "source": [
    "Df needs cleaning. Ultimate Goal is:  \n",
    "* Check for null values\n",
    "* check for empty spaces \n",
    "* check for nonsense texts \n",
    "* remove urls, https, special characters? \n",
    "* remove REUTERS()\n",
    "* group text by subject? and label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ecde8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check raw_df starting point\n",
    "raw_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a8d903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check null values\n",
    "raw_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6cd27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop missing values\n",
    "\n",
    "raw_df = raw_df.dropna()\n",
    "raw_df.shape\n",
    "\n",
    "# Note, dropna() not picking up null values as some text missing in DF. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ee0966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make [index] for rows that don't have publication info (REUTERS)\n",
    "# note: not all - are reuters, and may be in actual text\n",
    "\n",
    "\n",
    "no_publisher = []\n",
    "for index, row in enumerate(raw_df.text.values):\n",
    "    try:\n",
    "        record = row.split(\" -\", maxsplit=1)\n",
    "        \n",
    "        # if no text present, this should raise error \n",
    "        record[1]\n",
    "        assert(len(record[0]) < 260)\n",
    "    except:\n",
    "        no_publisher.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83ac084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for the 'clean' data, that doesn't start with REUTERS\n",
    "# note 23445 records\n",
    "\n",
    "raw_df.iloc[no_publisher].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff3a8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of indexes for rows that have publication info (REUTERS)\n",
    "# this will seperate the text from the publisher info (i.e seperate REUTERS())\n",
    "\n",
    "publisher = []\n",
    "pt_text=[]\n",
    "\n",
    "for index, row in enumerate(raw_df.text.values):\n",
    "    if index in no_publisher: \n",
    "        \n",
    "#         if no publisher mentioned, add unknown\n",
    "        pt_text.append(row)\n",
    "        publisher.append(\"Unknown\")\n",
    "        continue \n",
    "        \n",
    "    record = row.split(\" -\", maxsplit=1)\n",
    "    publisher.append(record[0])\n",
    "    pt_text.append(record[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0561da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace current text column with the new one made above \n",
    "# add new seperate column for publisher info (i.e REUTERS())\n",
    "\n",
    "raw_df[\"publisher\"] = publisher \n",
    "raw_df[\"text\"] = pt_text\n",
    "\n",
    "del publisher, pt_text, record, no_publisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a44b26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f304010",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.tail(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3564dbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for empty rows and list them\n",
    "#  note 631 empty values \n",
    "\n",
    "empty = [index for index, text in enumerate(raw_df.text.values) if str(text).strip()=='']\n",
    "print(f\"Number of empty rows: {len(empty)}\")\n",
    "raw_df.iloc[empty].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b6a93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  drop these empty row records \n",
    "\n",
    "# raw_df = raw_df.drop(empty, axis=0)\n",
    "# raw_df = raw_df[raw_df['text'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182242d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check empty values dropped\n",
    "# note still present in df \n",
    "raw_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b31109a",
   "metadata": {},
   "source": [
    "### Why isn't dropna working?  \n",
    "\n",
    "it now is after adding white space to empty text str."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1113642f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use numpy to replace white space with NaN function in order to pick up using dropna()\n",
    "\n",
    "raw_df['text'].replace(' ', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b8d69f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# confirm no empty rows \n",
    "# use drop method to check for empty spaces and not white spaces \n",
    "# note lost some values \n",
    "\n",
    "raw_df = raw_df.drop([index for index, text in enumerate(raw_df.text.values) if str(text).strip()==''])\n",
    "raw_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa70e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now can drop the null values using dropna \n",
    "# note lost more values  \n",
    "\n",
    "raw_df = raw_df.dropna(subset=['text'])\n",
    "raw_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3564d9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b22e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text has bool true so should return false if empty \n",
    "# note DF length matches above with 44263 rows\n",
    "\n",
    "raw_df['text'].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515fa512",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# raw_df = raw_df.drop(empty, inplace=True)\n",
    "# raw_df['text'].str.strip().astype(bool)\n",
    "# raw_df['text'].astype(bool)\n",
    "# df.dropna()\n",
    "# raw_df.iloc[empty].tail()\n",
    "# raw_df = (raw_df.iloc[empty]).replace('', np.nan)\n",
    "# print(raw_df)\n",
    "# print(type(df))\n",
    "# raw_df.empty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd624e7",
   "metadata": {},
   "source": [
    "## group by subject and label and analyze those \n",
    "## create histograms for subjects \n",
    "## create text count for label? or show article distributions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695e0469",
   "metadata": {},
   "source": [
    "## Let's clean up the DF "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bd0b74",
   "metadata": {},
   "source": [
    "Delete unecessary rows - publisher, date, title, subject \n",
    "They won't be needed going forwards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4248a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "del raw_df['title']\n",
    "del raw_df['subject']\n",
    "del raw_df['date']\n",
    "del raw_df['publisher']\n",
    "\n",
    "\n",
    "raw_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7fde0b",
   "metadata": {},
   "source": [
    "### Lets Clean up the data\n",
    "\n",
    "* Remove punctuation\n",
    "* remove special char \n",
    "* convert upper to lower \n",
    "* remove stopwords\n",
    "* remove urls\n",
    "* Lemmatization - The stemming of words without loss of meaning to context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d956091c",
   "metadata": {},
   "source": [
    "We'll clean the first news article only for now... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e30dd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_1 = raw_df['text'][0]\n",
    "type(text_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d792ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  use contractions lib for context when expanding contractions (i'd -> i would)\n",
    "# import contractions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6093dd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_1 = text_1.to_string()\n",
    "type(text_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbb9931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_df['text'] = raw_df['text'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c977f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# contractions now working after comnverting to str \n",
    "text_1 = ' '.join([contractions.fix(word) for word in text_1.split()])\n",
    "text_1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27704379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# will tokenizer work instead? as a secondary solution\n",
    "# apply word_tokenize function \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "tokens = raw_df['text'].apply(word_tokenize)\n",
    "tokens \n",
    "\n",
    "or \n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ac62a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#  remove stopwords \n",
    "#  is stopword library inclusive enough? \n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text_1 = nltk.word_tokenize(text_1) \n",
    "text_1 = [ word for word in text_1 if not word in set(stopwords.words(\"english\"))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658a7cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_1 = ' '.join(text_1)\n",
    "text_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6c5fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove special characters and punctuation \n",
    "import re \n",
    "import string \n",
    "\n",
    "text_1 = re.sub('\\[[^]]*\\]', ' ', text_1)\n",
    "text_1 = re.sub('[^a-zA-Z]', ' ', text_1)\n",
    "\n",
    "#  convert from  upper to lower \n",
    "text_1 = text_1.lower()\n",
    "\n",
    "text_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c100a33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  removal of HTML content\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(text_1, \"html.parser\")\n",
    "text_1 = soup.get_text()\n",
    "text_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7d284e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization \n",
    "\n",
    "lemma = nltk.WordNetLemmatizer()\n",
    "text_1 = [lemma.lemmatize(word) for word in text_1.split()]\n",
    "\n",
    "text_1 = \" \".join(text_1)\n",
    "text_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c996ab",
   "metadata": {},
   "source": [
    "* perhaps remove numbers? \n",
    "* what about exclamation and question marks? \n",
    "\n",
    "### Now lets use the tested cleansing above and make functions to  cleanse all text. add a few more functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c026735a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove URL\n",
    "def remove_URL(text):\n",
    "    url = re.compile(r'https?://\\S+')\n",
    "    return url.sub(r' httpsmark ', text)\n",
    "\n",
    "# HTML removal \n",
    "def remove_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "\n",
    "# Remove punctuation\n",
    "def remove_punct(text):\n",
    "    return re.sub('\\[[^]]*\\]', ' ', text)\n",
    "\n",
    "\n",
    "# remove special chars\n",
    "def remove_chars(text):\n",
    "    return re.sub(\"[^a-zA-A]\", \" \", text)\n",
    "\n",
    "# remove emoji \n",
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"  # dingbats\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r' emoji ', string)\n",
    "\n",
    "\n",
    "# remove stopwords\n",
    "def remove_stop_and_lemmat(text): \n",
    "    final_text = []\n",
    "    text = text.lower()\n",
    "    text = nltk.word_tokenize(text)\n",
    "    \n",
    "    for word in text:\n",
    "        if word not in set(stopwords.words('english')):\n",
    "            lemma = nltk.WordNetLemmatizer()\n",
    "            word = lemma.lemmatize(word)\n",
    "            final_text.append(word)\n",
    "    return \" \".join(final_text)\n",
    "\n",
    "\n",
    "# full function \n",
    "def cleanse(text):\n",
    "    text = remove_URL(text)\n",
    "    text = remove_html(text)\n",
    "    text = remove_punct(text)\n",
    "    text = remove_chars(text)\n",
    "    text = remove_emoji(text)\n",
    "    text = remove_stop_and_lemmat(text)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2849637",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Apply function to text col\n",
    "raw_df['text']=raw_df['text'].apply(cleanse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b1fd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180ee2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word cloud now with new cleaned data \n",
    "# STOPWORDS =set(nltk.corpus.stopwords.words(\"english\")).generate(text)\n",
    "\n",
    "plt.figure(figsize = (10,6))\n",
    "wc = WordCloud(width = 440, \n",
    "               height = 200, \n",
    "               stopwords =set(nltk.corpus.stopwords.words(\"english\")).generate(\" \".join(raw_df['text']))\n",
    "plt.imshow(wc, interpolation = 'bilinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f54fc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.tail(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124cc66d",
   "metadata": {},
   "source": [
    "\n",
    "# sentiment analysis - says if dataset is positive, neg or neutral\n",
    "# top to beck groups text into categories (topic model?)1\n",
    "# dashboard for visualisation (powerbi)\n",
    "# pull out a few words from cloud when describing analysis\n",
    "# use time frames to compare e.g. most pop topic in feb 2020 was\n",
    "# 30% train and 70% test for ML \n",
    "# topic model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
